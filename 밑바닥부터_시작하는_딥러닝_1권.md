*Chapter2 <퍼셉트론>*

- 퍼셉트론을 쌓으면 비선형 표현이 가능하다. -> 신경망

*Chapter3 <신경망>*

Activation function

- Sigmoid function
h(x) = 1/(1 + exp(-x))

- Step function
h(x) = if x > 0: 1 else 0

- 위의 둘 모두 비선형 함수임. 선형 함수를 사용하면 층을 깊게 쌓는 의미가 없어짐.

- Relu(Rectified Linear Unit) function <- 자주 쓰임.
h(x) = if x > 0: x else 0

- Softmax function(분모 출력 총합1, 함수의 출력은 확률로 해석)
$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}$ -> 지수 함수 때문에 overflow 문제 생김 -> $\text{softmax}(z_i) = \frac{e^{z_i - C}}{\sum_{j=1}^N e^{z_j - C}}, \quad \text{where } C = \max(z)$

출력층의 활성화 함수
- 회귀에는 항등 함수(입력값 그대로 출력).
- 2클래스 분류에는 시그모이도 함수.
- 다중 클래스 분류에는 소프트맥스 함수.

- 학습을 끝낸 실제 분류 시에는 출력층의 activation function 생략하는 경우도 있음. 어차피 최댓값의 순서는 달라지지 않기 때문.
- 분류 시 출력층의 뉴런 수는 분류할 레이블 개수와 같다.
- 출력 과정은 신경망의 순전파(forward propagation)라고 함.

- 입력 데이터를 묶은 것을 배치라 하며 추론 처리를 이 배치 단위로 진행하면 결과를 더 빠르게 얻을 수 있다.. 

*Chapter4 <신경망 학습>*

오차제곱합 SSE(Sum of squares for error)
- ![image](https://github.com/user-attachments/assets/5531db7b-dde6-4e94-9386-e62b26483fab)

교차 엔트로피 오차 CEE(Cross entropy error)
- ![image](https://github.com/user-attachments/assets/ab1e7054-e13e-429f-9f50-02c479be01f0)
- y는 정답 레이블, y_hat은 확률값
- 이 수식의 전제는 레이블이 원-핫 인코딩이 된 것이므로 정답일때는 y가 1, 오답일때는 y가 0임
