*Chapter2 <퍼셉트론>*

- 퍼셉트론을 쌓으면 비선형 표현이 가능하다. -> 신경망

*Chapter3 <신경망>*

Activation function

- Sigmoid function
h(x) = 1/(1 + exp(-x))

- Step function
h(x) = if x > 0: 1 else 0

- 위의 둘 모두 비선형 함수임. 선형 함수를 사용하면 층을 깊게 쌓는 의미가 없어짐.

- Relu(Rectified Linear Unit) function <- 자주 쓰임.
h(x) = if x > 0: x else 0

- Softmax function(분모 출력 총합1, 함수의 출력은 확률로 해석)
$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}$ -> 지수 함수 때문에 overflow 문제 생김 -> $\text{softmax}(z_i) = \frac{e^{z_i - C}}{\sum_{j=1}^N e^{z_j - C}}, \quad \text{where } C = \max(z)$

출력층의 활성화 함수
- 회귀에는 항등 함수(입력값 그대로 출력).
- 2클래스 분류에는 시그모이도 함수.
- 다중 클래스 분류에는 소프트맥스 함수.

- 학습을 끝낸 실제 분류 시에는 출력층의 activation function 생략하는 경우도 있음. 어차피 최댓값의 순서는 달라지지 않기 때문.
- 분류 시 출력층의 뉴런 수는 분류할 레이블 개수와 같다.
- 출력 과정은 신경망의 순전파(forward propagation)라고 함.

- 입력 데이터를 묶은 것을 배치라 하며 추론 처리를 이 배치 단위로 진행하면 결과를 더 빠르게 얻을 수 있다.. 

*Chapter4 <신경망 학습>*

오차제곱합 SSE(Sum of squares for error)
- ![image](https://github.com/user-attachments/assets/5531db7b-dde6-4e94-9386-e62b26483fab)

교차 엔트로피 오차 CEE(Cross entropy error)
- ![image](https://github.com/user-attachments/assets/ab1e7054-e13e-429f-9f50-02c479be01f0)
- y는 정답 레이블, y_hat은 확률값
- 이 수식의 전제는 레이블이 원-핫 인코딩이 된 것이므로 정답일때는 y가 1, 오답일때는 y가 0임
- ![image](https://github.com/user-attachments/assets/c27967bc-2c6e-46a0-b5df-5d0f26946bae)
- 이 수식은 위의 수식에서 N개의 데이터의 k번째 값을 의미함.

- 신경망을 학습할 때 정확도를 지표로 삼아서는 안 됨. 정확도를 지표로 하면 매개변수의 미분값이 대부분의 장소에서 0이 되기 때문. (Step function을 안 쓰는 것도 같은 맥락임.)

수치 미분(numerical differentiation, <-> 해석적(analytic) 미분): 아주 작은 차분으로 미분(사실상 단순 나눗셈해 근사)하는 것.
- 실제 미분 수식을 이용하면 반올림 오차(rounding error) 발생함.
- 해결책1: 보통 미분의 분모 h의 값은 10^-4를 이용하면 좋은 결과를 얻는다 알려짐.
- 해결책2: 중앙 차분 이용.

- 함수가 기울기가 0에 가깝게 되는 지점은 보통 극값과 안장점(Saddle point), 고원(Plateau)임.
- 안장점은 말 안장과 같은 모양이고 고원은 평평한 지점임.
- 학습률은 lr, learning rate라 부름.

*Chapter5 <오차역전파법>*

- 자세한 수식 설명은 나중에..

Affine 계층
- Y = XW + B 형태로 행렬곱에 덧셈이 첨가된 형태. 이를 affine 변환(transformation)이라고도 함.

*Chapter6 <학습 관련 기술들>*
