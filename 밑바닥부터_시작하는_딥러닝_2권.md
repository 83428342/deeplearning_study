*Chapter2<자연어와 단어의 분산 표현>*

- 자연어 처리(Natural Language Processing): 우리의 말을 컴퓨터에게 이해시키기 위한 기술.
- 컴퓨터에게 단어의 의미를 이해시키는 방식에는 시소러스 기법, 통계 기반 기법, 추론 기반 기법(word2vec)가 있음.

- 단어의 분산 표현: 단어를 고정 길이의 밀집벡터로 표현한 것(원-핫 인코딩 벡터 아님).

**시소러스**
- 동의어나 유의어를 한 그룹으로 분류하는 방식. (상위와 하위, 전체와 부분 등 더 세세한 관계까지 그래프 구조로 정의.)
- WordNet: 자연어 처리 분야에서 가장 유명한 시소러스.

문제점
- 시대 변화에 대응하기 어려움.
- 사람이 직접 레이블링해야해서 비용이 큼. (cf. 이미지 인식 분야에서도 예전에는 이 방식)
- 단어의 미묘한 차이를 표현할 수 없음.

- 특히 사람이 직접 의미를 정의해줘야 한다는 문제를 피하기 위해 통계 기반 기법과 추론 기반 기법을 사용함.

**통계 기반 기법**
- 말뭉치(Corpus): 대량의 텍스트 데이터.
- 통계 기반 기법은 말뭉치를 이용해 컴퓨터가 단어의 의미를 추출함. 정확히는 단어를 벡터로 변환.
- 말뭉치를 이용하기 위해서는 텍스트 데이터를 단어로 분할하고 해당 단어들을 단어 ID 목록으로 변환하는 전처리가 필요함.
- 단어의 분산 표현(distributional representation): 개별 단어를 고정 길이의 밀집벡터(대부분의 원소가 0이 아닌 벡터)로 표현함.

분포 가설(distributional hypothesis)
- 단어의 의미는 주변 단어에 의해 형성된다는 가설.
- 단어를 벡터로 표현하는 연구들은 이 가설에 기초함.
- 윈도우 크기(window size): 해당 단어의 맥락에 좌우 각각 몇 개의 단어를 포함시킬지에 대한 크기.

동시발생 행렬(Co-occurrence matrix)
- 특정 단어는 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하고 이를 벡터화하면 해당 단어를 벡터로 나타낼 수 있음.
- 이 집계를 위한 행렬이 동시발생 행렬임.

벡터 간 유사도
- 코사인 유사도를 이용해 측정 가능.

상호정보량
- 단순 동시발생 횟수만 따지면 관사와 같이 강한 관련이 없어도 자주 동시발생하는 단어가 관련성이 강하다고 잘못 평가될 수 있음 -> 점별 상호정보량(PMI: Pointwise Mutual Information)이용해 해결.
- ![image](https://github.com/user-attachments/assets/a528f481-dd91-4505-b996-ca3c37501fb7)
- PMI값이 높을수록 관련성이 높음.
- ![image](https://github.com/user-attachments/assets/f9572ecb-0d50-4cbd-9fda-afe758bcc5c6) 로도 나타낼 수 있음. 이 때 C는 동시발생행렬의 내부 변수의 발생 횟수, N은 말뭉치에 포함된 단어 수.
- 동시발생 횟수가 0이면 log0 = -무한대가 나옴 -> 양의 상호정보량(PPMI: Positive PMI)을 이용해 해결.
- ![image](https://github.com/user-attachments/assets/1baf22b7-3f19-4242-a0aa-69d3e307bc9a)

- 하지만 PPMI에도 여전히 큰 문제가 있음.. 말뭉치의 어휘 수가 증가함에 따라 단어 벡터의 차원 수가 증가한다는 문제임. -> 또한 희소 행렬이라는 문제도 있음. -> 차원 감소(dimensionality reduction)를 이용해 밀집벡터로 바꾸어 해결(주로 SVD사용).

- PTB 데이터셋: 벤치마크로 자주 이용되는 말뭉치 데이터셋.

- SVD 계산은 행렬의 크기가 N일 때 O(N^3)이 걸림. 이 때문에 Truncated SVD(특잇값이 작은 것을 버리는 방식)와 같은 더 빠른 기법을 사용함.

*Chapter3<word2vec>*

통계 기반 기법의 문제점
- 대규모 말뭉치에서 행렬이 너무 커짐(SVD 적용도 힘들어짐).
- 여러 번에 걸쳐 학습할 수 없고 단 한 번의 학습만 할 수 있음. 

**추론 기반 기법**
- 신경망을 이용해 주변 단어(맥락)가 주어졌을 때 단어(target)를 추론하는 기법.
- 통계 기반 기법과 달리 미니배치로 조금씩 학습하며 가중치 갱신 가능함.
- 신경망에서도 단어를 One-hot 처리한 벡터로 표현하며 단어의 수가 입력층 뉴런의 수가 됨.
- word2vec는 CBOW, skip-gram 모델 두 종류가 존재.

CBOW: continuous bag-of-words 모델(단순한 word2vec)
- ![image](https://github.com/user-attachments/assets/b927fc40-c865-4b63-bc35-8b18b3c5507b)
- 그림과 같이 두 개의 입력층이 있음. 입력값에 대한 다음 레이어의 출력은 평균값이 됨. <- 이는 윈도우 사이즈가 1이고 각각이 이전, 이후 단어 벡터를 입력으로 받기 때문임.
- 신경망의 가중치가 해당 단어의 분산 표현이 됨. -> 단어의 의미가 가중치에 녹아들어가있음.
- 은닉층의 뉴런 수는 입력층 뉴런 수보다 작게 해야 함. (정보 압축인 인코딩의 역할)
- ![image](https://github.com/user-attachments/assets/8c2361e1-987e-43bd-866b-80fafafd3e83) 다른 방식으로 그리면 이렇게 쓸 수도 있다.
- 실험 결과 CBOW(와 skip-gram 모델)는 단어의 분산 표현이 직관에 부합함.
- 단, 학습 시 어떤 분야의 말뭉치를 사용하냐에 따라 단어의 분산 표현이 다를 것임.
- 본질적으로 다중 분류이므로 softmax와 cross-entropy loss 사용.
- 단, 단어의 분산 표현으로는 입력 측 가중치와 출력 층 가중치 중 word2vec는 입력 측 가중치만 이용함. (비슷한 기법인 GloVe는 입출력 가중치를 모두 더하는것으로 선택함.)
- loss를 다음과 같이 확률로 표현할 수도 있음. ![image](https://github.com/user-attachments/assets/3d11991c-2864-455b-accf-8f9d8467b206)

skip-gram 모델
- 이전의 CBOW 모델이 주변 단어로부터 타깃을 추측했다면, skip-gram 모델은 중앙의 단어(타깃)으로부터 주변의 여러 단어(맥락)를 추측함.
- 따라서 입력층은 하나고 출력층은 추측하는 단어의 개수와 같음.
- ![image](https://github.com/user-attachments/assets/188b8926-55d5-4056-80bc-0f3bd09d1652)
- NNL을 적용한 손실 함수는 다음과 같음. ![image](https://github.com/user-attachments/assets/496bd0d3-edf0-4462-b89e-5530d05eb513)
- 일반적으로 CBOW와 skip-gram 모델 중에서는 skip-gram 모델의 성능이 단어 분산 표현의 정밀도가 더 좋은 경우가 많음. (특히 말뭉치가 커질수록 저빈도 단어나 유추 문제 성능 좋아짐)
- 하지만 학습 속도는 CBOW가 더 빠름. loss 계산이 더 간단하기 때문.

**통계 기반 vs 추론 기반**
- 통계 기반 기법은 주로 단어의 유사성이 인코딩됨.
- 추론 기반 기법의 word2vec(특히 skip-gram)는 단어의 유사성 + 단어 사이의 패턴까지 인코딩. -> 유추 문제도 풀 수 있음.
- 그러나 실제 성능은 비슷비슷하다..
- 추론 기반 기법과 통계 기반 기법은 수학적으로 어느 정도 연결되어있음.
- word2vec 이후 추론 기반 기법과 통계 기반 기법을 섞은 GloVe 기법이 등장함. (말뭉치 전체의 통계 정보를 손실함수에 도입해 미니배치 학습*Chapter2<자연어와 단어의 분산 표현>*

- 자연어 처리(Natural Language Processing): 우리의 말을 컴퓨터에게 이해시키기 위한 기술.
- 컴퓨터에게 단어의 의미를 이해시키는 방식에는 시소러스 기법, 통계 기반 기법, 추론 기반 기법(word2vec)가 있음.

**시소러스**
- 동의어나 유의어를 한 그룹으로 분류하는 방식. (상위와 하위, 전체와 부분 등 더 세세한 관계까지 그래프 구조로 정의.)
- WordNet: 자연어 처리 분야에서 가장 유명한 시소러스.

문제점
- 시대 변화에 대응하기 어려움.
- 사람이 직접 레이블링해야해서 비용이 큼. (cf. 이미지 인식 분야에서도 예전에는 이 방식)
- 단어의 미묘한 차이를 표현할 수 없음.

- 특히 사람이 직접 의미를 정의해줘야 한다는 문제를 피하기 위해 통계 기반 기법과 추론 기반 기법을 사용함.

**통계 기반 기법**
- 말뭉치(Corpus): 대량의 텍스트 데이터.
- 통계 기반 기법은 말뭉치를 이용해 컴퓨터가 단어의 의미를 추출함. 정확히는 단어를 벡터로 변환.
- 말뭉치를 이용하기 위해서는 텍스트 데이터를 단어로 분할하고 해당 단어들을 단어 ID 목록으로 변환하는 전처리가 필요함.
- 단어의 분산 표현(distributional representation): 개별 단어를 고정 길이의 밀집벡터(대부분의 원소가 0이 아닌 벡터)로 표현함.

분포 가설(distributional hypothesis)
- 단어의 의미는 주변 단어에 의해 형성된다는 가설.
- 단어를 벡터로 표현하는 연구들은 이 가설에 기초함.
- 윈도우 크기(window size): 해당 단어의 맥락에 좌우 각각 몇 개의 단어를 포함시킬지에 대한 크기.

동시발생 행렬(Co-occurrence matrix)
- 특정 단어는 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하고 이를 벡터화하면 해당 단어를 벡터로 나타낼 수 있음.
- 이 집계를 위한 행렬이 동시발생 행렬임.

벡터 간 유사도
- 코사인 유사도를 이용해 측정 가능.

상호정보량
- 단순 동시발생 횟수만 따지면 관사와 같이 강한 관련이 없어도 자주 동시발생하는 단어가 관련성이 강하다고 잘못 평가될 수 있음 -> 점별 상호정보량(PMI: Pointwise Mutual Information)이용해 해결.
- ![image](https://github.com/user-attachments/assets/a528f481-dd91-4505-b996-ca3c37501fb7)
- PMI값이 높을수록 관련성이 높음.
- ![image](https://github.com/user-attachments/assets/f9572ecb-0d50-4cbd-9fda-afe758bcc5c6) 로도 나타낼 수 있음. 이 때 C는 동시발생행렬의 내부 변수의 발생 횟수, N은 말뭉치에 포함된 단어 수.
- 동시발생 횟수가 0이면 log0 = -무한대가 나옴 -> 양의 상호정보량(PPMI: Positive PMI)을 이용해 해결.
- ![image](https://github.com/user-attachments/assets/1baf22b7-3f19-4242-a0aa-69d3e307bc9a)

- 하지만 PPMI에도 여전히 큰 문제가 있음.. 말뭉치의 어휘 수가 증가함에 따라 단어 벡터의 차원 수가 증가한다는 문제임. -> 또한 희소 행렬이라는 문제도 있음. -> 차원 감소(dimensionality reduction)를 이용해 밀집벡터로 바꾸어 해결(주로 SVD사용).

- PTB 데이터셋: 벤치마크로 자주 이용되는 말뭉치 데이터셋.

- SVD 계산은 행렬의 크기가 N일 때 O(N^3)이 걸림. 이 때문에 Truncated SVD(특잇값이 작은 것을 버리는 방식)와 같은 더 빠른 기법을 사용함.

*Chapter3<word2vec>*

통계 기반 기법의 문제점
- 대규모 말뭉치에서 행렬이 너무 커짐(SVD 적용도 힘들어짐).
- 여러 번에 걸쳐 학습할 수 없고 단 한 번의 학습만 할 수 있음. 

**추론 기반 기법**
- 신경망을 이용해 주변 단어(맥락)가 주어졌을 때 단어(target)를 추론하는 기법.
- 통계 기반 기법과 달리 미니배치로 조금씩 학습하며 가중치 갱신 가능함.
- 신경망에서도 단어를 One-hot 처리한 벡터로 표현하며 단어의 수가 입력층 뉴런의 수가 됨.
- word2vec는 CBOW, skip-gram 모델 두 종류가 존재.

CBOW: continuous bag-of-words 모델(단순한 word2vec)
- ![image](https://github.com/user-attachments/assets/b927fc40-c865-4b63-bc35-8b18b3c5507b)
- 그림과 같이 두 개의 입력층이 있음. 입력값에 대한 다음 레이어의 출력은 평균값이 됨. <- 이는 윈도우 사이즈가 1이고 각각이 이전, 이후 단어 벡터를 입력으로 받기 때문임.
- 신경망의 가중치가 해당 단어의 분산 표현이 됨. -> 단어의 의미가 가중치에 녹아들어가있음.
- 은닉층의 뉴런 수는 입력층 뉴런 수보다 작게 해야 함. (정보 압축인 인코딩의 역할)
- ![image](https://github.com/user-attachments/assets/8c2361e1-987e-43bd-866b-80fafafd3e83) 다른 방식으로 그리면 이렇게 쓸 수도 있다.
- 실험 결과 CBOW(와 skip-gram 모델)는 단어의 분산 표현이 직관에 부합함.
- 단, 학습 시 어떤 분야의 말뭉치를 사용하냐에 따라 단어의 분산 표현이 다를 것임.
- 본질적으로 다중 분류이므로 softmax와 cross-entropy loss 사용.
- 단, 단어의 분산 표현으로는 입력 측 가중치와 출력 층 가중치 중 word2vec는 입력 측 가중치만 이용함. (비슷한 기법인 GloVe는 입출력 가중치를 모두 더하는것으로 선택함.)
- loss를 다음과 같이 확률로 표현할 수도 있음. ![image](https://github.com/user-attachments/assets/3d11991c-2864-455b-accf-8f9d8467b206)

skip-gram 모델
- 이전의 CBOW 모델이 주변 단어로부터 타깃을 추측했다면, skip-gram 모델은 중앙의 단어(타깃)으로부터 주변의 여러 단어(맥락)를 추측함.
- 따라서 입력층은 하나고 출력층은 추측하는 단어의 개수와 같음.
- ![image](https://github.com/user-attachments/assets/188b8926-55d5-4056-80bc-0f3bd09d1652)
- NNL을 적용한 손실 함수는 다음과 같음. ![image](https://github.com/user-attachments/assets/496bd0d3-edf0-4462-b89e-5530d05eb513)
- 일반적으로 CBOW와 skip-gram 모델 중에서는 skip-gram 모델의 성능이 단어 분산 표현의 정밀도가 더 좋은 경우가 많음. (특히 말뭉치가 커질수록 저빈도 단어나 유추 문제 성능 좋아짐)
- 하지만 학습 속도는 CBOW가 더 빠름. loss 계산이 더 간단하기 때문.

**통계 기반 vs 추론 기반**
- 통계 기반 기법은 주로 단어의 유사성이 인코딩됨.
- 추론 기반 기법의 word2vec(특히 skip-gram)는 단어의 유사성 + 단어 사이의 패턴까지 인코딩. -> 유추 문제도 풀 수 있음.
- 그러나 실제 성능은 비슷비슷하다..
- 추론 기반 기법과 통계 기반 기법은 수학적으로 어느 정도 연결되어있음.
- word2vec 이후 추론 기반 기법과 통계 기반 기법을 섞은 GloVe 기법이 등장함. (말뭉치 전체의 통계 정보를 손실함수에 도입해 미니배치 학습)

- word2vec은 언어 모델용으로 개발된 것이 아닌 단어의 분산 표현을 얻는 도구로써 고안된 기법임.

*Chapter4<word2vec 속도 개선>*

- 앞 장의 word2vec는 단순한 구조로, 말뭉치의 어휘 수가 많아지면 계산량이 너무 커진다는 문제가 있음.
- 두 방식: 1. Embedding 계층의 도입(입력층의 벡터 크기 너무 큰 문제 해결)과 2. 네거티브 샘플링이라는 새로운 손실 함수(출력층의 계산량 너무 많은 문제 해결)를 도입해 개선할 수 있음.
- 핵심은 '일부'를 처리하는 방식으로 바꾸는 것.

**Embedding 계층**
- 입력층에서 입력 벡터와 가중치의 곱은 원-핫 벡터의 특성상 가중치 행렬의 특정 행이 그대로 출력됨 -> 사실 행렬 곱 계산은 필요없음.
- 이 말은 입력층의 가중치 행렬의 특징 행이 해당 원-핫 벡터가 나타내던 단어의 분산 표현 역할을 할 수 있음을 나타냄.
- 따라서 해당 단어 ID에 해당하는 행(벡터)를 추출하는 계층을 하나 만들어주기만 하면 됨 -> 이를 Embedding 계층이라 부름(단어의 유래는 word embedding으로, embedding 계층에 단어 임베딩, 즉 분산 표현을 저장하는 것이라 보면 됨.).
- 자연어 처리 분야에서 단어의 밀집벡터 표현을 단어 임베딩 혹을 단어의 분산 표현(distributed representation)이라 부름.
- 이를 통해 메모리 사용량을 줄이고 쓸데없는 계산도 생략 가능해짐.

**네거티브 샘플링**
- 다중 분류를 이진 분류로 근사하는 것이 핵심. -> Sigmoid를 이용함.
- Softmax 대신 사용하며, 어휘가 아무리 많아져도 계산량을 낮은 수준에서 일정하게 억제.
- 정답에 긍정적인 예시와 부정적인 예시를 몇개 골라(샘플링) loss들을 더해 최종 손실 값으로 사용함.
- 부정적 예시를 샘플링할때는 말뭉치에서 자주 등장하는 단어 위주로 추출함. (확률분포 이용, 단 출현 확률이 낮더라도 가끔은 샘플링되어야 하므로 확률분포를 수정한 식에 샘플링.)

**word2vec 남은 주제**
- 자연어 처리 분야의 단어의 분산 표현이 중요한 이유는 전이 학습(한 분야에서 배운 지식을 다른 분야에 적용하는 기법)에 있음.
- 단어의 분산 표현은 단어를 고정 길이 벡터로 변환해준다는 장점도 있음. -> 문장도 고정 길이 벡터로 변환 가능. ex. 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구함(bag-of-words, 단어의 순서는 고려x)
- 단어나 문장을 고정 길이 벡터로 변환하면 일반적 머신러닝 기법에 적용 가능해지므로 추가적 적용이 가능해짐.
- 단어의 분산 표현의 우수성은 실제 애플리케이션과는 분리해 평가함. 이 때 분산 표현의 우수성은 (코사인)유사성이나 유추 문제를 활용함.

*Chapter5<순환 신경망(RNN)>*

- 지금까지 살펴본 신경망은 feed forward 방식(흐름이 단방향)임.
- 그러나 이는 시계열 데이터를 잘 다루지 못한다는 단점이 있음 -> 시계열 데이터를 위한 RNN 도입.
- 사실 CBOW의 학습 목적은 맥락으로부터 타깃을 정확하게 추측하는 것이고, 그 부산물이 단어의 분산 표현이었음.
- 원래 목적인 맥락으로부터 타깃을 추측하는 것이 발전해 언어 모델이 쓰임.

**언어 모델(Language Model)**
- 단어 나열에 확률을 부여하는 모델. 즉, 특정한 단어의 시퀀스에 대해 그 시퀀스의 가능성을 확률로 평가함.
- 기계 번역과 음성 인식(사람의 음성으로부터 몇 개의 문장 생성 후 가장 자연스러운 문장 선택), 새로운 문장 생성이 대표적 예시.
- ![image](https://github.com/user-attachments/assets/b7476a6c-5b2b-42c3-9ae9-4af61cfadfc6) 처럼 수식 설명 가능(동시 확률)
- 즉, ![image](https://github.com/user-attachments/assets/91e641a5-98aa-400f-9648-2f637cd1e0dd)을 얻는 것이 목적이 됨. -> 이를 조건부 언어 모델(Conditional Language Model)이라 하기도 함.
- 마르코프 연쇄(Markov Chain): 미래가 현재 상태에만 의존해 결정되는 것. 직전 N개의 사건에 의존할 때 N층 마르코프 연쇄라 부름. (마르코프 모델: Markov Model이라고도 함.)

**RNN(Recurrent Neural Network)**
- 시계열 데이터의 인덱스를 가리킬 때는 '시각'이라 칭함(시점이라고도 함).
- 시각 t의 출력은 ![image](https://github.com/user-attachments/assets/e6d2fcc0-49dc-4be5-9414-0e15ac69a420)
- 이 때 ![image](https://github.com/user-attachments/assets/3bb2318e-8ccd-4ee1-832b-530110fdbe13)는 입력 벡터 x를 출력 h로 변환하는 가중치이고 ![image](https://github.com/user-attachments/assets/abea99f4-2ef6-42ab-99f4-a30716f34150)는 이전 시각의 출력을 다음 시각으로 넘겨줄 때 변환하는 가중치임.
- 즉, RNN은 h를 매 시각마다 갱신하며 이 h를 은닉 상태(hidden state) 또는 은닉 상태 벡터(hidden state vector)라 부름.

BPTT(Backpropagation Through Time)
- RNN에서의 일반적으로 사용하는 오차역전파법.
- RNN은 펼치면 매 시점마다 연결된 신경망으로 간주할 수 있으므로 RNN 학습은 뒤의 시점부터 차례대로 오차역전파법을 이용해 학습 가능함.
- 단, 시계열 데이터가 길어지면 BPTT의 컴퓨팅 자원 소비가 증가함. 또한 역전파 시의 기울기도 불안정해짐(ex. 길이가 1,000인 시계열 데이터를 다루며 RNN 계층을 펼치면 가로로 1,000개가 늘어선 신경망이 됨.). -> 여러 방법으로 극복(Truncated BPTT 등).

Truncated BPTT
- 큰 시계열 데이터를 취급할 때 시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라 작은 신경망 여러개로 만듦.
- 만들어진 작은 신경망 단위에서 오차역전파법 시행.
- 단, 역전파의 연결만 끊고 순전파의 연결은 유지함.

언어 모델의 평가
- 언어 모델은 주어진 과거 단어로부터 다음에 출현할 단어의 확률분포 출력
- 따라서 언어 모델의 예측 성능을 평가하는 척도로 Perplexity를 자주 이용.
- Perplexity: 확률의 역수라는 개념으로 받아들이면 편함. -> 값이 작을수록 성능이 좋은거임.
- 직관적으로는 다음에 출현할 수 있는 단어의 후보 수가 몇 개인지라고 보면 됨. 수식적으로는 아래와 같음.
- ![image](https://github.com/user-attachments/assets/32a24d95-4bcf-4435-b7d0-bd42eabac316)
- ![image](https://github.com/user-attachments/assets/6b6eebba-cc7a-4157-aa31-78308b908650)

*Chapter6<게이트가 추가된 RNN>*
- 기본적 구조의 RNN은 시계열 데이터에서 시간적으로 멀리 떨어진 장기(long term) 의존 관계를 잘 학습할 수 없어 성능이 좋지 못함.
- LSTM, GRU 등 RNN에 게이트(Gate)를 추가시킨 구조로 장기 의존 관계를 학습시킨 계층을 이용함.
- RMM이라고 하면 LSTM을 가리키는 경우도 흔함.

**RNN의 문제점**
- BPTT에서 기울기 소실(기울기가 점점 작아지다 사라짐) 혹은 기울기 폭발(기울기가 너무 큰 수가 됨)이 일어나기 때문에 시계열 데이터의 장기 의존 관계 학습이 어려움.
- 기울기 소실(Vanishing gradients)은 tanh함수의 미분값이 1보다 작거나 같기 때문에 멀리 있는 정보일수록 연쇄 법칙에 의해 값이 점점 작아져 발생.
- 기울기 폭발(Exploding gradients)은 역전파 시 행렬 곱셈에서 똑같은 가중치가 사용되는데, 이 때문에 기울기가 지수적으로 증가할 수 있음. -> 오버플로우 발생.
- 정리하면 곱해지는 행렬이 1보다 크면 지수적 증가(기울기 폭발), 1보다 작으면 지수적 감소(기울기 소실)
- 해당 행렬의 특잇값 중 최댓값이 1보다 큰지 여부를 봐 예측 가능. (1보다 크면 지수적 증가 예측 / 1보다 작으면 지수적 감소 예측 가능.)

기울기 폭발 대책
- 전통적으로 기울기 클리핑(Gradients Clipping) 기법을 이용함. pseudocode는 다음과 같음.
- ![image](https://github.com/user-attachments/assets/33a39d40-e9e2-4a3e-88eb-5062d7ee66d5)
- 꽤 잘 작동함.

기울기 소실과 LSTM
- 게이트(얼마나 정보를 흘려보낼지 조정하는 역할)를 추가해 해결.
- 대표적인 아키텍처가 LSTM과 GRU임.

**LSTM(Long Short-Term Memory)**
- ![image](https://github.com/user-attachments/assets/4794248d-03df-4ee8-9b21-6d8d513e6296)
- 여기서 C는 (기억) 셀(Memory Cell)이라 하며, LSTM 전용의 기억 메커니즘임.
- 기억 셀은 LSTM 내에서만 주고받음.
- 기억 셀에는 해당 시각에서의 LSTM의 기억이 저장되어 있음. 이 때 은닉 상태 h는 기억 셀의 값을 tanh함수로 변환한 것임.
- 정리하면 LSTM 내부에서 C_{t-1}, h_{t-1}, x_t 를 합쳐 적당한 계산을 하고 C_t의 값을 배출한 후 이를 tanh에 넣어 h_t를 배출하는 것.
- tanh에 게이트를 적용하는 건 tanh의 각 원소에 대해 '그것이 다음 시각의 은닉 상태에 얼마나 중요한가'를 조정하는 것.
- 해당 게이트는 h_t의 출력을 담당하는 게이트이므로 output 게이트(출력 게이트)라 함.
- 이 때 output은 ![image](https://github.com/user-attachments/assets/e8b3cfe6-2b40-44f2-86c4-b0d99e28899a) (Sigma는 시그모이드)
- 여기서 tanh가 데이터를 얼마나 통과시킬지 정하는 역할을 하게 됨.
- 또한 h_t는 output과 tanh의 원소별 곱으로 계산됨. (아다마르 곱: Hadamard product)
- 수식으로는 ![image](https://github.com/user-attachments/assets/221dab3a-4db6-4478-8842-cd6d25b8ba65)

- 망각 게이트(불필요한 정보를 잊게 해줌, forget 게이트)의 수식은 다음과 같음.
- ![image](https://github.com/user-attachments/assets/75dd3bcd-356a-48a1-96b9-5413379fcfc5)
- C_t는 C_{t-1}과 f의 아다마르 곱으로 구함.

- 그러나 이 상태로는 기억 셀이 잊는 것밖에 하지 못하므로 새로 기억해야 할 정보를 기억 셀에 추가하는 과정을 거쳐야 함.
- 따라서 ![image](https://github.com/user-attachments/assets/1e10f408-2eff-4b53-ba5d-a09079a5baff)를 추가함.

- 또한 g에 input 게이트(입력 게이트)를 추가하여 g의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지를 판단하게 함.
- 즉, 새 정보를 적절히 선택해 수용하게 함. (input 게이트에 의해 가중된 정보가 적당히 수정돼 새로 추가되는 것.)
- 이는 ![image](https://github.com/user-attachments/assets/b567eba1-c454-43a4-afe4-bfb29f467fb4)
- 마지막으로 i와 g의 원소별 곱 결과를 기억 셀에 추가.

- 최종적으로 다음과 같은 연산이 이루어짐. ![image](https://github.com/user-attachments/assets/8729afeb-9e8b-42e8-97a1-71facdaf5e49)

기울기 소실이 일어나지 않는 이유
- 기억 셀에서 + 노드는 기울기를 그대로 흘리고 x 노드에서는 행렬 곱이 아닌 아다마르 곱을 계산하는데, 원소별 곱이 일어나며 매 시각 다른 게이트 값을 이용하므로 곱셈 효과 누적이 일어나지 않아 기울기 소실이 일어나기 어려움.
- x 노드의 계산은 forget 게이트가 제어하는데, 잊어야 할 정보는 기울기가 작아지고 그렇지 않은 원소는 기울기가 작아지지 않은 채 과거 방향을 전해짐 -> 오래 기억해야 할 정보는 소실 없이 전파.
- 결국에는 게이트를 이용한 기억 셀 덕분에 기울기 소실을 막을 수 있음.

- PTB 데이터셋의 언어 모델에서는 LSTM의 층 수는 2~4 정도일 때 좋은 결과를 얻음.
- 구글 번역에서 사용하는 GNMT 모델은 LSTM을 8층으로 쌓음.

**드롭아웃에 의한 과적합 억제**
- LSTM 계층을 다층화하면 복잡한 표현력을 갖춘 모델을 만들 수 있지만 과적합을 일으킬 수 있음.
- RNN은 일반적인 피드포워드 신경망보다 쉽게 과적합을 일으킴.
- 드롭아웃 시 LSTM 계층의 시계열 방향으로 삽입하면 학습 시 정보가 사라질 수 있고 드롭아웃에 의한 노이즈가 축적되어 좋지 못함.
- 대신 LSTM 계층간의 상하관계 사이에 삽입하면 억제 가능.
- 하지만 최근 연구에서는 변형 드롭아웃(Variational Dropout)을 제안해 시간 방향으로 적용하는데 성공한 방법 등이 제안되고 있음. -> 깊이 방향, 시간 방향에 모두 드롭아웃을 적용해 정확도 상승 가능.
- 시간 축의 드롭아웃들은 같은 마스크(Mask: 데이터의 통과/차단을 결정하는 이진 형태의 무작위 패턴)를 공유해 마스크를 고정시킴. -> 정보를 잃게 되는 방법도 고정되어 일반적인 드롭아웃 방식과 달리 정보가 지수적으로 손실되지 않음.

**가중치 공유(Weight tying)**
- 언어 모델을 개선하는 아주 간단한 트릭.
- LSTM 계층 전후의 Embedding 계층과 Affine 계층이 가중치를 공유하는 등의 예시가 있음.
- '학습하는' 매개변수 수가 크게 줄어드는 동시에 정확도가 향상됨.

**GRU(Appendix C)**
- 게이트를 사용하나 매개변수를 줄여 계산 시간을 줄임.
- ![image](https://github.com/user-attachments/assets/4f9b5338-dbd5-4421-9663-2b65fc4a32aa)
- 위의 그림과 같이 기억 셀을 사용하지 않음.
- 계산 식은 아래와 같음.
- ![image](https://github.com/user-attachments/assets/fffd6e1a-1a22-4ff8-b116-ad4527649afb)
- 각각의 내부 계산은 아래와 같음.
- ![image](https://github.com/user-attachments/assets/7693f74d-0de5-48d8-a051-5aeb9e5c16fd)
- r은 reset 게이트, z는 update 게이트임.
- reset 게이트는 과거의 은닉 상태를 얼마나 무시할지 정하고 update 게이트는 은닉 상태 갱신(update 게이트가 LSTM의 forget 게이트와 input 게이트를 담당함.).
- 정리하면 GRU는 LSTM을 더 단순하게 만든 아키텍처라고 할 수 있음.
- 데이터셋이 작거나 모델 시도 시 반복 시도 많이할 때 LSTM보다 적합할 수 있음.


*Chapter7<RNN을 사용한 문장 생성>*

**seq2seq**
- 시계열 데이터를 다른 시계열 데이터로 변환하는 모델.
- Encoder-Decoder 모델이라고도 함.
- Encoder는 입력 데이터를 인코딩(부호화)하고 Decoder는 인코딩된 데이터를 디코딩(복호화)함.
- 입력 데이터가 Encoder에 주어지면 Encoder는 최종 은닉 상태(벡터)를 Decoder에 전달하고 Decoder가 최종 출력을 하게 됨.
- 각각은 LSTM으로 이루어진 신경망으로 구성됨.

가변 길이 시계열 데이터
- 샘플마다 데이터의 시간 방향 크기가 다르므로 패딩(Padding)을 이용해 모든 데이터의 길이를 균일하게 맞출 수 있음.
- 가장 긴 길이의 데이터의 길이에 모든 데이터의 길이를 늘림. 이 때 빈 공간으로(공백) 늘림.

- 일반적으로 Encoder와 Decoder의 Lstm 층수는 같게 해줌.

**seq2seq 개선**

입력 데이터 반전(Reverse)
- 아주 쉬운 트릭으로, 입력 데이터의 순서를 반전시키는 것.
- 학습 진행이 빨라지고 최종 정확도도 좋아짐.
- 직관적으로는 기울기 전파가 원활해지기 때문이라고 볼 수 있음.

엿보기(Peeky)
- Encoder의 출력 h를 Decoder의 최초 시각인 LSTM에만 전해주지 않고 그 뒤의(시각의) LSTM 계층에도 전해줌.

**seq2seq를 이용하는 애플리케이션**
- 기계 번역, 자동 요약, 질의응답, 메일 자동 응답
- 챗봇
- 알고리즘 학습
- 이미지 캡셔닝(Image Captioning: 이미지를 문장으로 변환): Encoder를 CNN 기반으로 바꾸어 최종 출력인 특징 맵을 Affine 계층에서 변환 후 기존 Decoder에 전달함.

*Chapter8<어텐션>*

**seq2seq의 문제점**
- Encoder는 입력 문장의 길이에 관계없이 정보를 고정 길이의 벡터로 압축해야함 -> 문제 발생.

**해결책**

Encoder 개선
- 따라서 Encoder 출력의 길이를 입력 문장의 길이에 따라 바꿔줌으로써 개선. (시각별 LSTM 계층의 은닉 상태 벡터를 모두 이용하면 됨.)
- 이 때 Eecoder의 출력 행렬의 벡터 각각은 직전 입력 데이터에 대한 정보가 가장 많이 포함되어있음 -> 해당 벡터는 해당 시점에 입력된 단어에 해당하는 벡터라고 보면 됨(하지만 단어 자체의 의미만 있는 것이 아닌 맥락 포함). -> 해당 데이터에는 이전 순서의 맥락만 포함되니 양방향 LSTM을 사용해서 개선할 수도 있음.

Decoder 개선
- 사진으로 나타내면 다음과 같이 계산한다.
- ![image](https://github.com/user-attachments/assets/321dbd8b-3649-44b7-8476-027195ebe6a6)
- 사진에서의 어떤 계산 층은 Attention 층임.

**어텐션**

- ![image](https://github.com/user-attachments/assets/cb340e3d-0ff1-412f-b35c-16a91f2d956c)

Weight Sum 계층
- Encoder가 출력하는 각 단어의 벡터가 모인 행렬 hs에서 해당 단어의 가중치 a를 구함.
- 즉, 해당 시점에서의 단어들의 중요도를 계산함.
- 이는 내적을 통해 softmax를 활용하여 각 단어의 가중치를 구하게 됨.

Attention Weight 계층
- a와 hs의 가중합을 구해 그 결과를 맥락 벡터 c로 출력.
- 이는 hs 각각의 벡터에 가중치 벡터 a를 대응되는 벡터마다 내적하여 맥락 벡터 c를 밷어냄.

**어텐션 관련 주제**

양방향 RNN(양방향 LSTM)
- ![image](https://github.com/user-attachments/assets/40867f6c-9fec-4bb1-8ac5-67cacb2b0416)
- 그림과 같이 역방향 처리 LSTM 계층을 추가시킴.
- 두 계층의 은닉 상태를 연결시킨 벡터를 최종 벡터로 출력. (연결 이외에도 합하거나 평균낼 수 있음.) 

seq2seq 심층화와 skip 연결
- attention 계층을 다른 곳에 배치하거나
- 잔차 연결을 통해 성능을 올려도 좋음.(깊이 방향 기울기 소실 해결)

구글 신경망 기계 번역(GNMT: Google Neural Machine Translation)
- seq2seq에 어텐션을 사용.
- LSTM 다층화, 양방향 LSTM(Encoder의 첫 층만), skip 연결 등을 이용함.

트랜스포머
- RNN을 사용하면 여러 단점이 있음 -> RNN을 배제하고 Attention만 사용한 기법.
- self-attention 기법이용.
- 일반적인 Attention은 왼쪽 그림, Self-Attention은 오른쪽 그림임.
- ![image](https://github.com/user-attachments/assets/5fcab3d5-e760-46a2-9129-ac68c3c47cfa)
- 트랜스포머의 계층 구성은 아래 그림과 같음.
- ![image](https://github.com/user-attachments/assets/841a13ac-9dfa-4744-bfc2-5ee2686e03fd)
- 여기서 Nx는 x번 쌓았다는 뜻.

뉴럴 튜링 머신(NTM)
- 외부 메모리를 통한 확장 이용.
- 궁금하면 논문을 보자..
