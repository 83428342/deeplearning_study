*Chapter2<자연어와 단어의 분산 표현>*

- 자연어 처리(Natural Language Processing): 우리의 말을 컴퓨터에게 이해시키기 위한 기술.
- 컴퓨터에게 단어의 의미를 이해시키는 방식에는 시소러스 기법, 통계 기반 기법, 추론 기반 기법(word2vec)가 있음.

**시소러스**
- 동의어나 유의어를 한 그룹으로 분류하는 방식. (상위와 하위, 전체와 부분 등 더 세세한 관계까지 그래프 구조로 정의.)
- WordNet: 자연어 처리 분야에서 가장 유명한 시소러스.

문제점
- 시대 변화에 대응하기 어려움.
- 사람이 직접 레이블링해야해서 비용이 큼. (cf. 이미지 인식 분야에서도 예전에는 이 방식)
- 단어의 미묘한 차이를 표현할 수 없음.

- 특히 사람이 직접 의미를 정의해줘야 한다는 문제를 피하기 위해 통계 기반 기법과 추론 기반 기법을 사용함.

**통계 기반 기법**
- 말뭉치(Corpus): 대량의 텍스트 데이터.
- 통계 기반 기법은 말뭉치를 이용해 컴퓨터가 단어의 의미를 추출함. 정확히는 단어를 벡터로 변환.
- 말뭉치를 이용하기 위해서는 텍스트 데이터를 단어로 분할하고 해당 단어들을 단어 ID 목록으로 변환하는 전처리가 필요함.
- 단어의 분산 표현(distributional representation): 개별 단어를 고정 길이의 밀집벡터(대부분의 원소가 0이 아닌 벡터)로 표현함.

분포 가설(distributional hypothesis)
- 단어의 의미는 주변 단어에 의해 형성된다는 가설.
- 단어를 벡터로 표현하는 연구들은 이 가설에 기초함.
- 윈도우 크기(window size): 해당 단어의 맥락에 좌우 각각 몇 개의 단어를 포함시킬지에 대한 크기.

동시발생 행렬(Co-occurrence matrix)
- 특정 단어는 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하고 이를 벡터화하면 해당 단어를 벡터로 나타낼 수 있음.
- 이 집계를 위한 행렬이 동시발생 행렬임.

벡터 간 유사도
- 코사인 유사도를 이용해 측정 가능.

상호정보량
- 단순 동시발생 횟수만 따지면 관사와 같이 강한 관련이 없어도 자주 동시발생하는 단어가 관련성이 강하다고 잘못 평가될 수 있음 -> 점별 상호정보량(PMI: Pointwise Mutual Information)이용해 해결.
- ![image](https://github.com/user-attachments/assets/a528f481-dd91-4505-b996-ca3c37501fb7)
- PMI값이 높을수록 관련성이 높음.
- ![image](https://github.com/user-attachments/assets/f9572ecb-0d50-4cbd-9fda-afe758bcc5c6) 로도 나타낼 수 있음. 이 때 C는 동시발생행렬의 내부 변수의 발생 횟수, N은 말뭉치에 포함된 단어 수.
- 동시발생 횟수가 0이면 log0 = -무한대가 나옴 -> 양의 상호정보량(PPMI: Positive PMI)을 이용해 해결.
- ![image](https://github.com/user-attachments/assets/1baf22b7-3f19-4242-a0aa-69d3e307bc9a)

- 하지만 PPMI에도 여전히 큰 문제가 있음.. 말뭉치의 어휘 수가 증가함에 따라 단어 벡터의 차원 수가 증가한다는 문제임. -> 또한 희소 행렬이라는 문제도 있음. -> 차원 감소(dimensionality reduction)를 이용해 밀집벡터로 바꾸어 해결(주로 SVD사용).

- PTB 데이터셋: 벤치마크로 자주 이용되는 말뭉치 데이터셋.

- SVD 계산은 행렬의 크기가 N일 때 O(N^3)이 걸림. 이 때문에 Truncated SVD(특잇값이 작은 것을 버리는 방식)와 같은 더 빠른 기법을 사용함.

*Chapter3<word2vec>*
