*Chapter2<자연어와 단어의 분산 표현>*

- 자연어 처리(Natural Language Processing): 우리의 말을 컴퓨터에게 이해시키기 위한 기술.
- 컴퓨터에게 단어의 의미를 이해시키는 방식에는 시소러스 기법, 통계 기반 기법, 추론 기반 기법(word2vec)가 있음.

- 단어의 분산 표현: 단어를 고정 길이의 밀집벡터로 표현한 것(원-핫 인코딩 벡터 아님).

**시소러스**
- 동의어나 유의어를 한 그룹으로 분류하는 방식. (상위와 하위, 전체와 부분 등 더 세세한 관계까지 그래프 구조로 정의.)
- WordNet: 자연어 처리 분야에서 가장 유명한 시소러스.

문제점
- 시대 변화에 대응하기 어려움.
- 사람이 직접 레이블링해야해서 비용이 큼. (cf. 이미지 인식 분야에서도 예전에는 이 방식)
- 단어의 미묘한 차이를 표현할 수 없음.

- 특히 사람이 직접 의미를 정의해줘야 한다는 문제를 피하기 위해 통계 기반 기법과 추론 기반 기법을 사용함.

**통계 기반 기법**
- 말뭉치(Corpus): 대량의 텍스트 데이터.
- 통계 기반 기법은 말뭉치를 이용해 컴퓨터가 단어의 의미를 추출함. 정확히는 단어를 벡터로 변환.
- 말뭉치를 이용하기 위해서는 텍스트 데이터를 단어로 분할하고 해당 단어들을 단어 ID 목록으로 변환하는 전처리가 필요함.
- 단어의 분산 표현(distributional representation): 개별 단어를 고정 길이의 밀집벡터(대부분의 원소가 0이 아닌 벡터)로 표현함.

분포 가설(distributional hypothesis)
- 단어의 의미는 주변 단어에 의해 형성된다는 가설.
- 단어를 벡터로 표현하는 연구들은 이 가설에 기초함.
- 윈도우 크기(window size): 해당 단어의 맥락에 좌우 각각 몇 개의 단어를 포함시킬지에 대한 크기.

동시발생 행렬(Co-occurrence matrix)
- 특정 단어는 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하고 이를 벡터화하면 해당 단어를 벡터로 나타낼 수 있음.
- 이 집계를 위한 행렬이 동시발생 행렬임.

벡터 간 유사도
- 코사인 유사도를 이용해 측정 가능.

상호정보량
- 단순 동시발생 횟수만 따지면 관사와 같이 강한 관련이 없어도 자주 동시발생하는 단어가 관련성이 강하다고 잘못 평가될 수 있음 -> 점별 상호정보량(PMI: Pointwise Mutual Information)이용해 해결.
- ![image](https://github.com/user-attachments/assets/a528f481-dd91-4505-b996-ca3c37501fb7)
- PMI값이 높을수록 관련성이 높음.
- ![image](https://github.com/user-attachments/assets/f9572ecb-0d50-4cbd-9fda-afe758bcc5c6) 로도 나타낼 수 있음. 이 때 C는 동시발생행렬의 내부 변수의 발생 횟수, N은 말뭉치에 포함된 단어 수.
- 동시발생 횟수가 0이면 log0 = -무한대가 나옴 -> 양의 상호정보량(PPMI: Positive PMI)을 이용해 해결.
- ![image](https://github.com/user-attachments/assets/1baf22b7-3f19-4242-a0aa-69d3e307bc9a)

- 하지만 PPMI에도 여전히 큰 문제가 있음.. 말뭉치의 어휘 수가 증가함에 따라 단어 벡터의 차원 수가 증가한다는 문제임. -> 또한 희소 행렬이라는 문제도 있음. -> 차원 감소(dimensionality reduction)를 이용해 밀집벡터로 바꾸어 해결(주로 SVD사용).

- PTB 데이터셋: 벤치마크로 자주 이용되는 말뭉치 데이터셋.

- SVD 계산은 행렬의 크기가 N일 때 O(N^3)이 걸림. 이 때문에 Truncated SVD(특잇값이 작은 것을 버리는 방식)와 같은 더 빠른 기법을 사용함.

*Chapter3<word2vec>*

통계 기반 기법의 문제점
- 대규모 말뭉치에서 행렬이 너무 커짐(SVD 적용도 힘들어짐).
- 여러 번에 걸쳐 학습할 수 없고 단 한 번의 학습만 할 수 있음. 

**추론 기반 기법**
- 신경망을 이용해 주변 단어(맥락)가 주어졌을 때 단어(target)를 추론하는 기법.
- 통계 기반 기법과 달리 미니배치로 조금씩 학습하며 가중치 갱신 가능함.
- 신경망에서도 단어를 One-hot 처리한 벡터로 표현하며 단어의 수가 입력층 뉴런의 수가 됨.
- word2vec는 CBOW, skip-gram 모델 두 종류가 존재.

CBOW: continuous bag-of-words 모델(단순한 word2vec)
- ![image](https://github.com/user-attachments/assets/b927fc40-c865-4b63-bc35-8b18b3c5507b)
- 그림과 같이 두 개의 입력층이 있음. 입력값에 대한 다음 레이어의 출력은 평균값이 됨. <- 이는 윈도우 사이즈가 1이고 각각이 이전, 이후 단어 벡터를 입력으로 받기 때문임.
- 신경망의 가중치가 해당 단어의 분산 표현이 됨. -> 단어의 의미가 가중치에 녹아들어가있음.
- 은닉층의 뉴런 수는 입력층 뉴런 수보다 작게 해야 함. (정보 압축인 인코딩의 역할)
- ![image](https://github.com/user-attachments/assets/8c2361e1-987e-43bd-866b-80fafafd3e83) 다른 방식으로 그리면 이렇게 쓸 수도 있다.
- 실험 결과 CBOW(와 skip-gram 모델)는 단어의 분산 표현이 직관에 부합함.
- 단, 학습 시 어떤 분야의 말뭉치를 사용하냐에 따라 단어의 분산 표현이 다를 것임.
- 본질적으로 다중 분류이므로 softmax와 cross-entropy loss 사용.
- 단, 단어의 분산 표현으로는 입력 측 가중치와 출력 층 가중치 중 word2vec는 입력 측 가중치만 이용함. (비슷한 기법인 GloVe는 입출력 가중치를 모두 더하는것으로 선택함.)
- loss를 다음과 같이 확률로 표현할 수도 있음. ![image](https://github.com/user-attachments/assets/3d11991c-2864-455b-accf-8f9d8467b206)

skip-gram 모델
- 이전의 CBOW 모델이 주변 단어로부터 타깃을 추측했다면, skip-gram 모델은 중앙의 단어(타깃)으로부터 주변의 여러 단어(맥락)를 추측함.
- 따라서 입력층은 하나고 출력층은 추측하는 단어의 개수와 같음.
- ![image](https://github.com/user-attachments/assets/188b8926-55d5-4056-80bc-0f3bd09d1652)
- NNL을 적용한 손실 함수는 다음과 같음. ![image](https://github.com/user-attachments/assets/496bd0d3-edf0-4462-b89e-5530d05eb513)
- 일반적으로 CBOW와 skip-gram 모델 중에서는 skip-gram 모델의 성능이 단어 분산 표현의 정밀도가 더 좋은 경우가 많음. (특히 말뭉치가 커질수록 저빈도 단어나 유추 문제 성능 좋아짐)
- 하지만 학습 속도는 CBOW가 더 빠름. loss 계산이 더 간단하기 때문.

**통계 기반 vs 추론 기반**
- 통계 기반 기법은 주로 단어의 유사성이 인코딩됨.
- 추론 기반 기법의 word2vec(특히 skip-gram)는 단어의 유사성 + 단어 사이의 패턴까지 인코딩. -> 유추 문제도 풀 수 있음.
- 그러나 실제 성능은 비슷비슷하다..
- 추론 기반 기법과 통계 기반 기법은 수학적으로 어느 정도 연결되어있음.
- word2vec 이후 추론 기반 기법과 통계 기반 기법을 섞은 GloVe 기법이 등장함. (말뭉치 전체의 통계 정보를 손실함수에 도입해 미니배치 학습*Chapter2<자연어와 단어의 분산 표현>*

- 자연어 처리(Natural Language Processing): 우리의 말을 컴퓨터에게 이해시키기 위한 기술.
- 컴퓨터에게 단어의 의미를 이해시키는 방식에는 시소러스 기법, 통계 기반 기법, 추론 기반 기법(word2vec)가 있음.

**시소러스**
- 동의어나 유의어를 한 그룹으로 분류하는 방식. (상위와 하위, 전체와 부분 등 더 세세한 관계까지 그래프 구조로 정의.)
- WordNet: 자연어 처리 분야에서 가장 유명한 시소러스.

문제점
- 시대 변화에 대응하기 어려움.
- 사람이 직접 레이블링해야해서 비용이 큼. (cf. 이미지 인식 분야에서도 예전에는 이 방식)
- 단어의 미묘한 차이를 표현할 수 없음.

- 특히 사람이 직접 의미를 정의해줘야 한다는 문제를 피하기 위해 통계 기반 기법과 추론 기반 기법을 사용함.

**통계 기반 기법**
- 말뭉치(Corpus): 대량의 텍스트 데이터.
- 통계 기반 기법은 말뭉치를 이용해 컴퓨터가 단어의 의미를 추출함. 정확히는 단어를 벡터로 변환.
- 말뭉치를 이용하기 위해서는 텍스트 데이터를 단어로 분할하고 해당 단어들을 단어 ID 목록으로 변환하는 전처리가 필요함.
- 단어의 분산 표현(distributional representation): 개별 단어를 고정 길이의 밀집벡터(대부분의 원소가 0이 아닌 벡터)로 표현함.

분포 가설(distributional hypothesis)
- 단어의 의미는 주변 단어에 의해 형성된다는 가설.
- 단어를 벡터로 표현하는 연구들은 이 가설에 기초함.
- 윈도우 크기(window size): 해당 단어의 맥락에 좌우 각각 몇 개의 단어를 포함시킬지에 대한 크기.

동시발생 행렬(Co-occurrence matrix)
- 특정 단어는 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하고 이를 벡터화하면 해당 단어를 벡터로 나타낼 수 있음.
- 이 집계를 위한 행렬이 동시발생 행렬임.

벡터 간 유사도
- 코사인 유사도를 이용해 측정 가능.

상호정보량
- 단순 동시발생 횟수만 따지면 관사와 같이 강한 관련이 없어도 자주 동시발생하는 단어가 관련성이 강하다고 잘못 평가될 수 있음 -> 점별 상호정보량(PMI: Pointwise Mutual Information)이용해 해결.
- ![image](https://github.com/user-attachments/assets/a528f481-dd91-4505-b996-ca3c37501fb7)
- PMI값이 높을수록 관련성이 높음.
- ![image](https://github.com/user-attachments/assets/f9572ecb-0d50-4cbd-9fda-afe758bcc5c6) 로도 나타낼 수 있음. 이 때 C는 동시발생행렬의 내부 변수의 발생 횟수, N은 말뭉치에 포함된 단어 수.
- 동시발생 횟수가 0이면 log0 = -무한대가 나옴 -> 양의 상호정보량(PPMI: Positive PMI)을 이용해 해결.
- ![image](https://github.com/user-attachments/assets/1baf22b7-3f19-4242-a0aa-69d3e307bc9a)

- 하지만 PPMI에도 여전히 큰 문제가 있음.. 말뭉치의 어휘 수가 증가함에 따라 단어 벡터의 차원 수가 증가한다는 문제임. -> 또한 희소 행렬이라는 문제도 있음. -> 차원 감소(dimensionality reduction)를 이용해 밀집벡터로 바꾸어 해결(주로 SVD사용).

- PTB 데이터셋: 벤치마크로 자주 이용되는 말뭉치 데이터셋.

- SVD 계산은 행렬의 크기가 N일 때 O(N^3)이 걸림. 이 때문에 Truncated SVD(특잇값이 작은 것을 버리는 방식)와 같은 더 빠른 기법을 사용함.

*Chapter3<word2vec>*

통계 기반 기법의 문제점
- 대규모 말뭉치에서 행렬이 너무 커짐(SVD 적용도 힘들어짐).
- 여러 번에 걸쳐 학습할 수 없고 단 한 번의 학습만 할 수 있음. 

**추론 기반 기법**
- 신경망을 이용해 주변 단어(맥락)가 주어졌을 때 단어(target)를 추론하는 기법.
- 통계 기반 기법과 달리 미니배치로 조금씩 학습하며 가중치 갱신 가능함.
- 신경망에서도 단어를 One-hot 처리한 벡터로 표현하며 단어의 수가 입력층 뉴런의 수가 됨.
- word2vec는 CBOW, skip-gram 모델 두 종류가 존재.

CBOW: continuous bag-of-words 모델(단순한 word2vec)
- ![image](https://github.com/user-attachments/assets/b927fc40-c865-4b63-bc35-8b18b3c5507b)
- 그림과 같이 두 개의 입력층이 있음. 입력값에 대한 다음 레이어의 출력은 평균값이 됨. <- 이는 윈도우 사이즈가 1이고 각각이 이전, 이후 단어 벡터를 입력으로 받기 때문임.
- 신경망의 가중치가 해당 단어의 분산 표현이 됨. -> 단어의 의미가 가중치에 녹아들어가있음.
- 은닉층의 뉴런 수는 입력층 뉴런 수보다 작게 해야 함. (정보 압축인 인코딩의 역할)
- ![image](https://github.com/user-attachments/assets/8c2361e1-987e-43bd-866b-80fafafd3e83) 다른 방식으로 그리면 이렇게 쓸 수도 있다.
- 실험 결과 CBOW(와 skip-gram 모델)는 단어의 분산 표현이 직관에 부합함.
- 단, 학습 시 어떤 분야의 말뭉치를 사용하냐에 따라 단어의 분산 표현이 다를 것임.
- 본질적으로 다중 분류이므로 softmax와 cross-entropy loss 사용.
- 단, 단어의 분산 표현으로는 입력 측 가중치와 출력 층 가중치 중 word2vec는 입력 측 가중치만 이용함. (비슷한 기법인 GloVe는 입출력 가중치를 모두 더하는것으로 선택함.)
- loss를 다음과 같이 확률로 표현할 수도 있음. ![image](https://github.com/user-attachments/assets/3d11991c-2864-455b-accf-8f9d8467b206)

skip-gram 모델
- 이전의 CBOW 모델이 주변 단어로부터 타깃을 추측했다면, skip-gram 모델은 중앙의 단어(타깃)으로부터 주변의 여러 단어(맥락)를 추측함.
- 따라서 입력층은 하나고 출력층은 추측하는 단어의 개수와 같음.
- ![image](https://github.com/user-attachments/assets/188b8926-55d5-4056-80bc-0f3bd09d1652)
- NNL을 적용한 손실 함수는 다음과 같음. ![image](https://github.com/user-attachments/assets/496bd0d3-edf0-4462-b89e-5530d05eb513)
- 일반적으로 CBOW와 skip-gram 모델 중에서는 skip-gram 모델의 성능이 단어 분산 표현의 정밀도가 더 좋은 경우가 많음. (특히 말뭉치가 커질수록 저빈도 단어나 유추 문제 성능 좋아짐)
- 하지만 학습 속도는 CBOW가 더 빠름. loss 계산이 더 간단하기 때문.

**통계 기반 vs 추론 기반**
- 통계 기반 기법은 주로 단어의 유사성이 인코딩됨.
- 추론 기반 기법의 word2vec(특히 skip-gram)는 단어의 유사성 + 단어 사이의 패턴까지 인코딩. -> 유추 문제도 풀 수 있음.
- 그러나 실제 성능은 비슷비슷하다..
- 추론 기반 기법과 통계 기반 기법은 수학적으로 어느 정도 연결되어있음.
- word2vec 이후 추론 기반 기법과 통계 기반 기법을 섞은 GloVe 기법이 등장함. (말뭉치 전체의 통계 정보를 손실함수에 도입해 미니배치 학습)

- word2vec은 언어 모델용으로 개발된 것이 아닌 단어의 분산 표현을 얻는 도구로써 고안된 기법임.

*Chapter4<word2vec 속도 개선>*

- 앞 장의 word2vec는 단순한 구조로, 말뭉치의 어휘 수가 많아지면 계산량이 너무 커진다는 문제가 있음.
- 두 방식: 1. Embedding 계층의 도입(입력층의 벡터 크기 너무 큰 문제 해결)과 2. 네거티브 샘플링이라는 새로운 손실 함수(출력층의 계산량 너무 많은 문제 해결)를 도입해 개선할 수 있음.
- 핵심은 '일부'를 처리하는 방식으로 바꾸는 것.

**Embedding 계층**
- 입력층에서 입력 벡터와 가중치의 곱은 원-핫 벡터의 특성상 가중치 행렬의 특정 행이 그대로 출력됨 -> 사실 행렬 곱 계산은 필요없음.
- 이 말은 입력층의 가중치 행렬의 특징 행이 해당 원-핫 벡터가 나타내던 단어의 분산 표현 역할을 할 수 있음을 나타냄.
- 따라서 해당 단어 ID에 해당하는 행(벡터)를 추출하는 계층을 하나 만들어주기만 하면 됨 -> 이를 Embedding 계층이라 부름(단어의 유래는 word embedding으로, embedding 계층에 단어 임베딩, 즉 분산 표현을 저장하는 것이라 보면 됨.).
- 자연어 처리 분야에서 단어의 밀집벡터 표현을 단어 임베딩 혹을 단어의 분산 표현(distributed representation)이라 부름.
- 이를 통해 메모리 사용량을 줄이고 쓸데없는 계산도 생략 가능해짐.

**네거티브 샘플링**
- 다중 분류를 이진 분류로 근사하는 것이 핵심. -> Sigmoid를 이용함.
- Softmax 대신 사용하며, 어휘가 아무리 많아져도 계산량을 낮은 수준에서 일정하게 억제.
- 정답에 긍정적인 예시와 부정적인 예시를 몇개 골라(샘플링) loss들을 더해 최종 손실 값으로 사용함.
- 부정적 예시를 샘플링할때는 말뭉치에서 자주 등장하는 단어 위주로 추출함. (확률분포 이용, 단 출현 확률이 낮더라도 가끔은 샘플링되어야 하므로 확률분포를 수정한 식에 샘플링.)

**word2vec 남은 주제**
- 자연어 처리 분야의 단어의 분산 표현이 중요한 이유는 전이 학습(한 분야에서 배운 지식을 다른 분야에 적용하는 기법)에 있음.
- 단어의 분산 표현은 단어를 고정 길이 벡터로 변환해준다는 장점도 있음. -> 문장도 고정 길이 벡터로 변환 가능. ex. 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구함(bag-of-words, 단어의 순서는 고려x)
- 단어나 문장을 고정 길이 벡터로 변환하면 일반적 머신러닝 기법에 적용 가능해지므로 추가적 적용이 가능해짐.
- 단어의 분산 표현의 우수성은 실제 애플리케이션과는 분리해 평가함. 이 때 분산 표현의 우수성은 (코사인)유사성이나 유추 문제를 활용함.

*Chapter5<순환 신경망(RNN)>*

- 지금까지 살펴본 신경망은 feed forward 방식(흐름이 단방향)임.
- 그러나 이는 시계열 데이터를 잘 다루지 못한다는 단점이 있음 -> 시계열 데이터를 위한 RNN 도입.
- 사실 CBOW의 학습 목적은 맥락으로부터 타깃을 정확하게 추측하는 것이고, 그 부산물이 단어의 분산 표현이었음.
- 원래 목적인 맥락으로부터 타깃을 추측하는 것이 발전해 언어 모델이 쓰임.

**언어 모델(Language Model)**
- 단어 나열에 확률을 부여하는 모델. 즉, 특정한 단어의 시퀀스에 대해 그 시퀀스의 가능성을 확률로 평가함.
- 기계 번역과 음성 인식(사람의 음성으로부터 몇 개의 문장 생성 후 가장 자연스러운 문장 선택), 새로운 문장 생성이 대표적 예시.
- ![image](https://github.com/user-attachments/assets/b7476a6c-5b2b-42c3-9ae9-4af61cfadfc6) 처럼 수식 설명 가능(동시 확률)
- 즉, ![image](https://github.com/user-attachments/assets/91e641a5-98aa-400f-9648-2f637cd1e0dd)을 얻는 것이 목적이 됨. -> 이를 조건부 언어 모델(Conditional Language Model)이라 하기도 함.
- 마르코프 연쇄(Markov Chain): 미래가 현재 상태에만 의존해 결정되는 것. 직전 N개의 사건에 의존할 때 N층 마르코프 연쇄라 부름. (마르코프 모델: Markov Model이라고도 함.)

**RNN(Recurrent Neural Network)**
- 시계열 데이터의 인덱스를 가리킬 때는 '시각'이라 칭함(시점이라고도 함).
- 시각 t의 출력은 ![image](https://github.com/user-attachments/assets/e6d2fcc0-49dc-4be5-9414-0e15ac69a420)
- 이 때 ![image](https://github.com/user-attachments/assets/3bb2318e-8ccd-4ee1-832b-530110fdbe13)는 입력 벡터 x를 출력 h로 변환하는 가중치이고 ![image](https://github.com/user-attachments/assets/abea99f4-2ef6-42ab-99f4-a30716f34150)는 이전 시각의 출력을 다음 시각으로 넘겨줄 때 변환하는 가중치임.
- 즉, RNN은 h를 매 시각마다 갱신하며 이 h를 은닉 상태(hidden state) 또는 은닉 상태 벡터(hidden state vector)라 부름.

BPTT(Backpropagation Through Time)
- RNN에서의 일반적으로 사용하는 오차역전파법.
- RNN은 펼치면 매 시점마다 연결된 신경망으로 간주할 수 있으므로 RNN 학습은 뒤의 시점부터 차례대로 오차역전파법을 이용해 학습 가능함.
- 단, 시계열 데이터가 길어지면 BPTT의 컴퓨팅 자원 소비가 증가함. 또한 역전파 시의 기울기도 불안정해짐(ex. 길이가 1,000인 시계열 데이터를 다루며 RNN 계층을 펼치면 가로로 1,000개가 늘어선 신경망이 됨.). -> 여러 방법으로 극복(Truncated BPTT 등).

Truncated BPTT
- 큰 시계열 데이터를 취급할 때 시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라 작은 신경망 여러개로 만듦.
- 만들어진 작은 신경망 단위에서 오차역전파법 시행.
- 단, 역전파의 연결만 끊고 순전파의 연결은 유지함.

언어 모델의 평가
- 언어 모델은 주어진 과거 단어로부터 다음에 출현할 단어의 확률분포 출력
- 따라서 언어 모델의 예측 성능을 평가하는 척도로 Perplexity를 자주 이용.
- Perplexity: 확률의 역수라는 개념으로 받아들이면 편함. -> 값이 작을수록 성능이 좋은거임.
- 직관적으로는 다음에 출현할 수 있는 단어의 후보 수가 몇 개인지라고 보면 됨. 수식적으로는 아래와 같음.
- ![image](https://github.com/user-attachments/assets/32a24d95-4bcf-4435-b7d0-bd42eabac316)
- ![image](https://github.com/user-attachments/assets/6b6eebba-cc7a-4157-aa31-78308b908650)

*Chapter6<게이트가 추가된 RNN>*
